# -*- coding: utf-8 -*-
"""NN_HW_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j9hwGg-9-ItlbGhS_BfQiKzZyFWcbxeT
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
import os.path
from keras import layers
from keras import models
from keras.utils.np_utils import to_categorical
from keras.regularizers import l1
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dropout

"""# Load dataset"""

# Access to google drive
from google.colab import drive
drive.mount('/content/gdrive')

!unrar x '/content/gdrive/MyDrive/Colab Notebooks/SceneClassificationData.rar'

"""# CNN

## Preprocess
"""

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    'SceneClassificationData/seg_train',
    target_size=(60, 60),
    batch_size=32,
    color_mode='rgb',
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_directory(
    'SceneClassificationData/seg_validation',
    target_size=(60, 60),
    batch_size=32,
    color_mode='rgb',
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    'SceneClassificationData/seg_test/seg_test',
    target_size=(60, 60),
    batch_size=32,
    color_mode='rgb',
    class_mode='categorical'
)

for data_batch, labels_batch in train_generator:
  print('data batch shape:', data_batch.shape)
  print('labels batch shape:', labels_batch.shape)
  break

"""## Model"""

def design_model():
  # Architecture
  model = models.Sequential()
  ## convnet
  model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(60,60,3)))
  model.add(layers.MaxPooling2D((2,2)))

  model.add(layers.Conv2D(64, (3,3), activation='relu'))
  model.add(layers.MaxPooling2D((2,2)))

  model.add(layers.Conv2D(128, (3,3), activation='relu'))
  model.add(layers.MaxPooling2D((2,2)))

  model.add(layers.Conv2D(128, (3,3), activation='relu'))
  model.add(layers.MaxPooling2D((2,2)))

  ## add a classifier on top
  model.add(layers.Flatten())
  model.add(layers.Dense(512, activation='relu'))
  model.add(Dropout(0.5))
  model.add(layers.Dense(6, activation='softmax'))


  # Compilation
  model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
  return model

model = design_model()
model.summary()

# Training
history = model.fit(train_generator,
                    steps_per_epoch=100,
                    epochs=25,
                    validation_data=val_generator,
                    validation_steps=50)
model.save('model.h5')

test_loss, test_acc = model.evaluate(test_generator)
print('Loss of testset (Cross Entropy):', test_loss)
print('Metric of testset (Accuracy):', test_acc)

"""## Plot"""

history_dict = history.history
history_dict.keys()

# Plot training and validation Loss

train_loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(train_loss_values)+1)

plt.plot(epochs, train_loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training and validation Accuracy

train_acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
epochs = range(1, len(train_acc_values)+1)

plt.plot(epochs, train_acc_values, 'bo', label='Training acc')
plt.plot(epochs, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""**It seems that best epoch for this problem is 16**

## Prediction
"""

pred_datagen = ImageDataGenerator(rescale=1./255)

pred_generator = pred_datagen.flow_from_directory(
    'SceneClassificationData/seg_pred',
    target_size=(60, 60),
    batch_size=32,
    color_mode='rgb',
    class_mode='categorical'
)

yhat = model.predict(pred_generator)

predictions = []
for y in yhat:
  class_pred = np.argmax(y)
  predictions.append(class_pred)

"""# Data Augmentation

## Preprocess
"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=5,
    width_shift_range=0.2,
    zoom_range=0.2,
    brightness_range=[0.2,0.8],
    horizontal_flip=True)

train_generator = train_datagen.flow_from_directory(
    'SceneClassificationData/seg_train',
    target_size=(60, 60),
    batch_size=32,
    color_mode='rgb',
    class_mode='categorical'
)

"""## Train"""

# Compilation
model.compile(optimizer='rmsprop',
            loss='categorical_crossentropy',
            metrics=['accuracy'])

# Training
history = model.fit(train_generator,
                    steps_per_epoch=100,
                    epochs=25,
                    validation_data=val_generator,
                    validation_steps=50)
model.save('model_aug.h5')

test_loss, test_acc = model.evaluate(test_generator)
print('Loss of testset (Cross Entropy):', test_loss)
print('Metric of testset (Accuracy):', test_acc)

"""## Plot"""

history_dict = history.history
history_dict.keys()

# Plot training and validation Loss

train_loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(train_loss_values)+1)

plt.plot(epochs, train_loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training and validation Accuracy

train_acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
epochs = range(1, len(train_acc_values)+1)

plt.plot(epochs, train_acc_values, 'bo', label='Training acc')
plt.plot(epochs, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""## Prediction"""

pred_datagen_aug = ImageDataGenerator(rescale=1./255)

pred_generator_aug = pred_datagen_aug.flow_from_directory(
    'SceneClassificationData/seg_pred',
    target_size=(60, 60),
    batch_size=32,
    color_mode='rgb',
    class_mode='categorical'
)

yhat_aug = model.predict(pred_generator_aug)

predictions_aug = []
for y in yhat_aug:
  class_pred = np.argmax(y)
  predictions_aug.append(class_pred)